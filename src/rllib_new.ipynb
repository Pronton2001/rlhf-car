{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from l5kit.data.map_api import MapAPI\n",
    "# from l5kit.environment.envs.l5_env import GymStepOutput, SimulationConfigGym\n",
    "from l5kit.environment.envs.l5_env2 import GymStepOutput, SimulationConfigGym, L5Env2\n",
    "from l5kit.environment.envs.l5_env import GymStepOutput, SimulationConfigGym, L5Env\n",
    "# Dataset is assumed to be on the folder specified\n",
    "# in the L5KIT_DATA_FOLDER environment variable\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.logger import pretty_print\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from wrapper import L5EnvWrapper\n",
    "# from src.customEnv.wr import wrapper\n",
    "# from customModel.customModel import TorchGNCNN, TorchAttentionModel, TorchAttentionModel2\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tempfile import gettempdir\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDatasetVectorized\n",
    "from l5kit.planning.vectorized.closed_loop_model import VectorizedUnrollModel\n",
    "from l5kit.planning.vectorized.open_loop_model import VectorizedModel\n",
    "from l5kit.vectorization.vectorizer_builder import build_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get environment config\n",
    "from src.constant import SRC_PATH\n",
    "\n",
    "\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/workspace/datasets\"\n",
    "env_config_path =  SRC_PATH  + 'src/configs/gym_vectorizer_config.yaml'\n",
    "# env_config_path = SRC_PATH  + 'src/configs/gym_rasterizer_config.yaml'\n",
    "# env_config_path = SRC_PATH + 'src/configs/gym_vectorizer_config_hist3.yaml'\n",
    "dmg = LocalDataManager(None)\n",
    "cfg = load_config_data(env_config_path)\n",
    "###############\n",
    "USE_KIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_path = \"/home/pronton/rl/l5kit/examples/urban_driver/OL_HS.pt\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = torch.load(model_path).to(device)\n",
    "# # model = SAC.load(\"/home/pronton/rl/l5kit/examples/RL/gg colabs/logs/SAC_640000_steps.zip\")\n",
    "# model = model.eval()\n",
    "# torch.set_grad_enabled(False)\n",
    "from src.customModel.customModel import TorchAttentionModel3, TorchAttentionModel4\n",
    "from src.customModel.customModel import TorchRasterNet\n",
    "\n",
    "\n",
    "# os.environ[\"L5KIT_DATA_FOLDER\"] = \"/media/pronton/linux_files/a100code/l5kit/l5kit_dataset\"\n",
    "# env_config_path = '/home/pronton/rl/rlhf-car/src/configs/gym_vectorizer_config.yaml'\n",
    "dmg = LocalDataManager(None)\n",
    "cfg = load_config_data(env_config_path)\n",
    "# rollout_sim_cfg = SimulationConfigGym()\n",
    "# rollout_sim_cfg.num_simulation_steps = None\n",
    "# env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': False, 'sim_cfg': rollout_sim_cfg,  'train': False, 'return_info': True, 'rescale_action': False}\n",
    "# rollout_env = L5Env2(**env_kwargs)\n",
    "# print(rollout_env.action_space)\n",
    "# model = TorchAttentionModel3(np.zeros((112,112,7)), np.array((3,)),3, model_config= {\"custom_model_config\": {'cfg':cfg}}, name='')\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "###################### TRAINING ######################\n",
    "ModelCatalog.register_custom_model( \"TorchSeparatedAttentionModel\", TorchAttentionModel3)\n",
    "ModelCatalog.register_custom_model( \"TorchAttentionModel4\", TorchAttentionModel4)\n",
    "ModelCatalog.register_custom_model( \"TorchSeparatedRasterModel\", TorchRasterNet)\n",
    "from src.customEnv.wrapper import L5Env2WrapperTorchCLEReward, L5EnvRasterizerTorch\n",
    "from ray import tune\n",
    "import ray\n",
    "train_eps_length = 32\n",
    "train_sim_cfg = SimulationConfigGym()\n",
    "train_sim_cfg.num_simulation_steps = train_eps_length + 1\n",
    "env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': USE_KIN, 'sim_cfg': train_sim_cfg}\n",
    "reward_kwargs = {\n",
    "    'yaw_weight': 1.0,\n",
    "    'dist_weight': 1.0,\n",
    "    'cf_weight': 20.0,\n",
    "    'cr_weight': 20.0,\n",
    "    'cs_weight': 20.0,\n",
    "}\n",
    "tune.register_env(\"L5-CLE-V2\", lambda config: L5Env2WrapperTorchCLEReward(L5Env2(**env_kwargs), reward_kwargs=reward_kwargs))\n",
    "tune.register_env(\"L5-CLE-V1\", lambda config: L5EnvRasterizerTorch(env = L5Env(**env_kwargs), \\\n",
    "                                                           raster_size= cfg['raster_params']['raster_size'][0], \\\n",
    "                                                           n_channels = 5))\n",
    "if USE_KIN:\n",
    "    kin_rescale =  L5Env2(**env_kwargs).kin_rescale\n",
    "    print('non_kin_rescale:', kin_rescale)\n",
    "else:\n",
    "    non_kin_rescale =  L5Env2(**env_kwargs).non_kin_rescale\n",
    "    print('non_kin_rescale:', non_kin_rescale)\n",
    "\n",
    "\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True, log_to_driver=False, local_mode=True)\n",
    "from src.customModel.customKLRewardPPOTrainer import KLPPO\n",
    "algo = ppo.PPO(\n",
    "# algo = KLPPO(\n",
    "        env=\"L5-CLE-V2\", #\"L5-CLE-V2\",\n",
    "        config={\n",
    "            # 'disable_env_checking':True,\n",
    "            \"framework\": \"torch\",\n",
    "            'log_level': 'INFO',\n",
    "            'num_gpu': 0,\n",
    "            'train_batch_size': 1,\n",
    "            'sgd_minibatch_size': 1,\n",
    "            'num_sgd_iter': 1,\n",
    "            'seed': 42,\n",
    "            'batch_mode': 'truncate_episodes',\n",
    "            # \"rollout_fragment_length\": 32,\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"TorchAttentionModel4\", #TorchSeparatedAttentionModel\n",
    "                # Extra kwargs to be passed to your model's c'tor.\n",
    "                \"custom_model_config\": {\n",
    "                    'cfg': cfg,\n",
    "                    'freeze_for_RLtuning':  True,\n",
    "                    'load_pretrained': True,\n",
    "                    'shared_feature_extractor': True,\n",
    "                    'KL_pretrained': True,\n",
    "                    # 'future_num_frames':cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                    # 'freeze_actor': True,\n",
    "                    # 'non_kin_rescale': non_kin_rescale,\n",
    "                    },\n",
    "            },            \n",
    "            # \"output\": \"/home/pronton/rl/l5kit/examples/RL/notebooks/logs/l5env2-out\", \n",
    "            # \"output_max_file_size\": 5000000,\n",
    "            '_disable_preprocessor_api': True,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./BPTT.pt\"\n",
    "model_path =  f\"{SRC_PATH}src/model/OL_HS.pt\"\n",
    "device = 'cuda:0'\n",
    "model = torch.load(model_path).to(device)\n",
    "model = model.eval()\n",
    "# torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.customEnv.action_utils import inverseUnicycle, standard_normalizer_nonKin, standard_normalizer_kin\n",
    "\n",
    "def rollout_episode(model, env, idx = 0, model_type = 'OPENED_LOOP'):\n",
    "    \"\"\"Rollout a particular scene index and return the simulation output.\n",
    "\n",
    "    :param model: the RL policy\n",
    "    :param env: the gym environment\n",
    "    :param idx: the scene index to be rolled out\n",
    "    :return: the episode output of the rolled out scene\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the reset_scene_id to 'idx'\n",
    "    env.set_reset_id(idx)\n",
    "    \n",
    "    # Rollout step-by-step\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while True:\n",
    "        # print(obs)\n",
    "        # action = np.array(env.action_space.sample())\n",
    "        if model_type == 'CLOSED_LOOP':\n",
    "            action = model.compute_single_action(obs)\n",
    "        if model_type == 'OPENED_LOOP':\n",
    "            if type(obs) == dict:\n",
    "                obs = {k: torch.as_tensor(v).view(1, *torch.as_tensor(v).shape).to(device) for k, v in obs.items()}\n",
    "                # print(obs['agent_polyline_availability'])\n",
    "                # for  k, v in obs.items():\n",
    "                    # if k == 'agent_polyline_availability':\n",
    "                    #     obs[k] = torch.as_tensor(v.shape[0] * [True, True, False, False], dtype = bool).view(v.shape[0], 4)\n",
    "            elif type(obs) == np.ndarray:\n",
    "                obs = torch.as_tensor(obs).view(1, *obs.shape)\n",
    "            logits = model(obs)\n",
    "            if type(logits) == dict: # TODO: Change Vectorized output from dict -> numpy.ndarray\n",
    "                pred_x = logits['positions'][:,0, 0].view(-1,1)# take the first action \n",
    "                pred_y = logits['positions'][:,0, 1].view(-1,1)# take the first action\n",
    "                pred_yaw = logits['yaws'][:,0,:].view(-1,1)# take the first action\n",
    "            else:\n",
    "                batch_size = len(obs)\n",
    "                predicted = logits.view(batch_size, -1, 3) # B, N, 3 (X,Y,yaw)\n",
    "                pred_x = predicted[:, 0, 0].view(-1,1) # take the first action \n",
    "                pred_y = predicted[:, 0, 1].view(-1,1) # take the first action\n",
    "                pred_yaw = predicted[:, 0, 2].view(-1,1)# take the first action\n",
    "            action = torch.cat((pred_x,pred_y, pred_yaw), dim = -1).detach().cpu().numpy()\n",
    "            # Normalize kin actions\n",
    "            if USE_KIN:\n",
    "                action = inverseUnicycle(pred_x, pred_y, pred_yaw, obs['old_speed']) # B, 1\n",
    "                # print(f'inverse Unicycle: {output_logits}')\n",
    "                action = standard_normalizer_kin(action).detach().numpy().reshape(-1) # scale actions\n",
    "                # print(f'normalize actions: {action}')\n",
    "            else:\n",
    "                action = standard_normalizer_nonKin(action).reshape(-1)\n",
    "\n",
    "        # print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(action)\n",
    "        # print(env.ego_output_dict)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # The episode outputs are present in the key \"sim_outs\"\n",
    "    sim_out = info[\"sim_outs\"][0]\n",
    "    return sim_out, np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_sim_cfg = SimulationConfigGym()\n",
    "rollout_sim_cfg.num_simulation_steps = None\n",
    "\n",
    "env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': USE_KIN, 'sim_cfg': rollout_sim_cfg,  'train': True, 'return_info': True}#, 'rescale_action': True}\n",
    "# tune.register_env(\"L5-CLE-V0\", lambda config: L5Env(**env_kwargs))\n",
    "rollout_env = L5EnvRasterizerTorch(env = L5Env(**env_kwargs), \\\n",
    "                                                           raster_size= cfg['raster_params']['raster_size'][0], \\\n",
    "                                                           n_channels = 5)\n",
    "# rollout_env = L5Env2(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.customEnv.wrapper import L5Env2WrapperTorchCLEReward, L5EnvRasterizerTorch\n",
    "from ray import tune\n",
    "rollout_sim_cfg = SimulationConfigGym()\n",
    "rollout_sim_cfg.num_simulation_steps = None\n",
    "eval_env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': USE_KIN, 'sim_cfg': rollout_sim_cfg, 'train': True, 'return_info': True}\n",
    "rollout_env = L5Env2WrapperTorchCLEReward(L5Env2(**eval_env_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "model_path =  f\"{SRC_PATH}src/model/OL_HS.pt\"\n",
    "model = VectorizedModel(\n",
    "    history_num_frames_ego=cfg[\"model_params\"][\"history_num_frames_ego\"],\n",
    "    history_num_frames_agents=cfg[\"model_params\"][\"history_num_frames_agents\"],\n",
    "    num_targets=36, # N (X,Y,Yaw) 36\n",
    "    weights_scaling=[1,1,1], # 3\n",
    "    criterion=nn.L1Loss(reduction=\"none\"),\n",
    "    global_head_dropout=cfg[\"model_params\"][\"global_head_dropout\"],\n",
    "    disable_other_agents=cfg[\"model_params\"][\"disable_other_agents\"],\n",
    "    disable_map=cfg[\"model_params\"][\"disable_map\"],\n",
    "    disable_lane_boundaries=cfg[\"model_params\"][\"disable_lane_boundaries\"])\n",
    "pretrained_model = torch.load(model_path)\n",
    "pretrained_model.eval()\n",
    "for param_tensor in pretrained_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", pretrained_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path).state_dict())\n",
    "model.eval()\n",
    "\n",
    "print('-------------')\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "torch.set_grad_enabled(False)\n",
    "# for  name, param in model.named_parameters():\n",
    "#     print(model[name], model[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from l5kit.visualization.visualizer.zarr_utils import episode_out_to_visualizer_scene_gym_cle, simulation_out_to_visualizer_scene\n",
    "from l5kit.visualization.visualizer.visualizer import visualize\n",
    "from bokeh.io import output_notebook, show\n",
    "sim_outs =[]\n",
    "rollout_sim_cfg = SimulationConfigGym()\n",
    "rollout_sim_cfg.num_simulation_steps = None\n",
    "# env_config_path = '/workspace/source/src/configs/gym_config.yaml'\n",
    "\n",
    "# env_config_path = \n",
    "# env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': False, 'sim_cfg': rollout_sim_cfg,  'train': False, 'return_info': True}\n",
    "\n",
    "# rollout_env = L5Env(**env_kwargs)\n",
    "\n",
    "# for i in range(1):\n",
    "# sim_outs.append(rollout_episode(algo, rollout_env, 82, KLPPO))\n",
    "# results =  np.array([rollout_episode(algo, rollout_env, i, 'CLOSED_LOOP') for i in [7,8,34,13]])\n",
    "# results = np.array([rollout_episode(algo.get_policy().model.pretrained_policy, rollout_env, i, 'OPENED_LOOP') for i in [70]])\n",
    "results = np.array([rollout_episode(model, rollout_env, i, 'OPENED_LOOP') for i in [70]])\n",
    "sim_outs,rewards = results[:,0], np.array(results[:,1])\n",
    "rewards.reshape(rewards.shape[0], -1)\n",
    "print([np.sum(r) for r in rewards])\n",
    "\n",
    "# might change with different rasterizer\n",
    "mapAPI = MapAPI.from_cfg(dmg, cfg)\n",
    "\n",
    "def visualize_outputs(sim_outs, map_API):\n",
    "    for sim_out in sim_outs: # for each scene\n",
    "        vis_in = episode_out_to_visualizer_scene_gym_cle(sim_out, map_API)\n",
    "        show(visualize(sim_out.scene_id, vis_in))\n",
    "\n",
    "output_notebook()\n",
    "visualize_outputs(sim_outs, mapAPI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L5env1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eps_length = 32\n",
    "train_sim_cfg = SimulationConfigGym()\n",
    "train_sim_cfg.num_simulation_steps = train_eps_length + 1\n",
    "env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': False, 'sim_cfg': train_sim_cfg}\n",
    "\n",
    "\n",
    "tune.register_env(\"L5-CLE-V1\", lambda config: L5EnvWrapper(L5Env(**env_kwargs)))\n",
    "ray.init(num_cpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"tcnn\", TorchGNCNN\n",
    "    )\n",
    "# Create the Trainer.\n",
    "algo = ppo.PPO(\n",
    "        env=\"L5-CLE-V1\",\n",
    "        config={\n",
    "            'num_worker': 1,\n",
    "            'disable_env_checking':True,\n",
    "            \"framework\": \"torch\",\n",
    "            # \"model\": {\n",
    "            #     \"custom_model\": \"GN_CNN_torch_model\",\n",
    "            #     \"custom_model_config\": {'feature_dim':128},\n",
    "            # },\n",
    "            # \"output\": \"/home/pronton/rl/l5kit/examples/RL/notebooks/logs/l5env2-out\", \n",
    "            # \"output_max_file_size\": 5000000,\n",
    "            '_disable_preprocessor_api': True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "for i in range(2):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_eps_length = 10\n",
    "train_sim_cfg = SimulationConfigGym()\n",
    "train_sim_cfg.num_simulation_steps = train_eps_length + 1\n",
    "\n",
    "env_kwargs = {'env_config_path': env_config_path, 'use_kinematic': True, 'sim_cfg': train_sim_cfg}\n",
    "# env = L5Env2(**env_kwargs)\n",
    "# env1_kwargs = {'env_config_path': env_config_path, 'use_kinematic': True, 'sim_cfg': train_sim_cfg}\n",
    "# env1 = L5Env(**env_kwargs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L5env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algo = ppo.PPO(\n",
    "        env=\"L5-CLE-V2\",\n",
    "        config={\n",
    "            'disable_env_checking':True,\n",
    "            \"framework\": \"torch\",\n",
    "            'log_level': 'INFO',\n",
    "            'num_gpu': 1,\n",
    "            'train_batch_size': 1,\n",
    "            'sgd_minibatch_size': 1,\n",
    "            'num_sgd_iter': 1,\n",
    "            'seed': 42,\n",
    "            'batch_mode': 'truncate_episodes',\n",
    "            # \"rollout_fragment_length\": 32,\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"TorchSharedAttentionModel\",\n",
    "                # Extra kwargs to be passed to your model's c'tor.\n",
    "                \"custom_model_config\": {'cfg':cfg},\n",
    "            },\n",
    "            # \"output\": \"/home/pronton/rl/l5kit/examples/RL/notebooks/logs/l5env2-out\", \n",
    "            # \"output_max_file_size\": 5000000,\n",
    "            '_disable_preprocessor_api': True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "for i in range(1):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6df7c2a3d813445d6b3c74a479f8d37af444dbb4628cead36b7b0d6872de20bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
