{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29a0818-02ab-465a-be54-1440896fd474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 09:50:41,700\tWARNING deprecation.py:47 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "2023-02-11 09:50:41,701\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\n",
      "2023-02-11 09:50:41,709\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/DATA/demo-out/output-2023-02-11_09-50-41_worker-0_0.json' mode='w' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preprocessor is <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f6c03165160>\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ray._private.utils\n",
    "\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_builder = SampleBatchBuilder()  # or MultiAgentSampleBatchBuilder\n",
    "    writer = JsonWriter(\n",
    "        os.path.join('.', \"demo-out\")\n",
    "    )\n",
    "\n",
    "    # You normally wouldn't want to manually create sample batches if a\n",
    "    # simulator is available, but let's do it anyways for example purposes:\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "    # RLlib uses preprocessors to implement transforms such as one-hot encoding\n",
    "    # and flattening of tuple and dict observations. For CartPole a no-op\n",
    "    # preprocessor is used, but this may be relevant for more complex envs.\n",
    "    prep = get_preprocessor(env.observation_space)(env.observation_space)\n",
    "    print(\"The preprocessor is\", prep)\n",
    "\n",
    "    for eps_id in range(100):\n",
    "        obs = env.reset()\n",
    "        prev_action = np.zeros_like(env.action_space.sample())\n",
    "        prev_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, rew, done, info = env.step(action)\n",
    "            batch_builder.add_values(\n",
    "                t=t,\n",
    "                eps_id=eps_id,\n",
    "                agent_index=0,\n",
    "                obs=prep.transform(obs),\n",
    "                actions=action,\n",
    "                action_prob=1.0,  # put the true action probability here\n",
    "                action_logp=0.0,\n",
    "                rewards=rew,\n",
    "                prev_actions=prev_action,\n",
    "                prev_rewards=prev_reward,\n",
    "                dones=done,\n",
    "                infos=info,\n",
    "                new_obs=prep.transform(new_obs),\n",
    "            )\n",
    "            obs = new_obs\n",
    "            prev_action = action\n",
    "            prev_reward = rew\n",
    "            t += 1\n",
    "        writer.write(batch_builder.build_and_reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f4487d-c438-40b3-a4ac-e111b7f5a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-11 15:20:18,281\tWARNING services.py:1732 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 66695168 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2023-02-11 15:20:19,429\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(PG pid=1261856)\u001b[0m 2023-02-11 15:20:25,645\tWARNING algorithm_config.py:488 -- Cannot create PGConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "== Status ==\n",
      "Current time: 2023-02-11 15:20:26 (running for 00:00:03.91)\n",
      "Memory usage on this node: 161.9/503.2 GiB \n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/64 CPUs, 0/1 GPUs, 0.0/448.68 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(PG pid=1261856)\u001b[0m 2023-02-11 15:20:25,955\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PG pid=1261856)\u001b[0m 2023-02-11 15:20:26,009\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "== Status ==\n",
      "Current time: 2023-02-11 15:20:26 (running for 00:00:04.34)\n",
      "Memory usage on this node: 161.9/503.2 GiB \n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/64 CPUs, 0/1 GPUs, 0.0/448.68 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "2023-02-11 15:20:26,816\tINFO tune.py:762 -- Total run time: 4.88 seconds (4.33 seconds for the tuning loop).\n",
      "\n",
      "Your training finished.\n",
      "Best available checkpoint for each trial:\n",
      "  \u001b[35m/root/ray_results/default/PG_Pendulum-v1_99cc1_00000_0_2023-02-11_15-20-22/\u001b[0m\u001b[95mche\u001b[0m\n",
      "\u001b[95mckpoint_000001\u001b[0m\n",
      "\n",
      "You can now evaluate your trained algorithm from any checkpoint, e.g. by \n",
      "running:\n",
      "╭──────────────────────────────────────────────────────────────────────────────╮\n",
      "│ \u001b[32m  rllib evaluate \u001b[0m                                                            │\n",
      "│ \u001b[32m/root/ray_results/default/PG_Pendulum-v1_99cc1_00000_0_2023-02-11_15-20-22/c\u001b[0m │\n",
      "│ \u001b[32mheckpoint_000001 --algo PG\u001b[0m                                                   │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!rllib train \\\n",
    "    --run=PG \\\n",
    "    --env=Pendulum-v1 \\\n",
    "     --framework=torch \\\n",
    "    --config='{\"output\": \"/DATA/pendulum-out\", \"output_max_file_size\": 100}' \\\n",
    "    --stop='{\"timesteps_total\": 100}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155c573d-215e-434f-a483-57e58dc196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "\n",
    "from ray.rllib.policy.sample_batch import convert_ma_batch_to_sample_batch\n",
    "from ray.rllib.algorithms import cql as cql\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.offline.estimators import (\n",
    "    ImportanceSampling,\n",
    "    WeightedImportanceSampling,\n",
    "    DirectMethod,\n",
    "    DoublyRobust,\n",
    ")\n",
    "from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel\n",
    "\n",
    "\n",
    "torch, _ = try_import_torch()\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     \"--as-test\",\n",
    "#     action=\"store_true\",\n",
    "#     help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "#     \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--stop-iters\", type=int, default=5, help=\"Number of iterations to train.\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--stop-reward\", type=float, default=50.0, help=\"Reward at which we stop training.\"\n",
    "# )\n",
    "hcmTz = pytz.timezone(\"Asia/Ho_Chi_Minh\") \n",
    "date = datetime.datetime.now(hcmTz).strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "ray_result_logdir = '/DATA/l5kit/ray_results/' + date\n",
    "\n",
    "\n",
    "stop_iters = 5\n",
    "stop_reward=50\n",
    "as_test=True\n",
    "runs = 'CQL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84e2de7-e1d0-436b-b129-e5be31d71dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 12:34:19,175\tWARNING services.py:1732 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 66695168 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2023-02-11 12:34:20,329\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '172.17.0.2', 'raylet_ip_address': '172.17.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-02-11_12-34-17_280152_1233260/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-02-11_12-34-17_280152_1233260/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-02-11_12-34-17_280152_1233260', 'metrics_export_port': 63694, 'gcs_address': '172.17.0.2:45310', 'address': '172.17.0.2:45310', 'dashboard_agent_listen_port': 52365, 'node_id': '7618a9c5ec9c23c0259c261d5ae5bace0cb1c504c93c13382028cb5c'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import ray\n",
    "ray.init(num_cpus=64, ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d553d1ea-ee9c-4fb8-be1c-b69872d9f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    cql.CQLConfig()\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(num_rollout_workers=62)\n",
    "    .resources(num_gpus = 1)\n",
    "    .training(\n",
    "        n_step=3,\n",
    "        bc_iters=0,\n",
    "        clip_actions=False,\n",
    "        tau=0.005,\n",
    "        target_entropy=\"auto\",\n",
    "        q_model_config={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        policy_model_config={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        optimization_config={\n",
    "            \"actor_learning_rate\": 3e-4,\n",
    "            \"critic_learning_rate\": 3e-4,\n",
    "            \"entropy_learning_rate\": 3e-4,\n",
    "        },\n",
    "        train_batch_size=256,\n",
    "        target_network_update_freq=1,\n",
    "        num_steps_sampled_before_learning_starts=256,\n",
    "    )\n",
    "    .reporting(min_train_timesteps_per_iteration=1000)\n",
    "    .debugging(log_level=\"INFO\")\n",
    "    .environment(normalize_actions=True, env=\"Pendulum-v1\")\n",
    "    # .offline_data(\n",
    "    #     input_config={\n",
    "    #         \"paths\": [\"./demo-out/output-2023-02-11_06-03-52_worker-0_0.json\"], #tests/data/pendulum/enormous.zip\n",
    "    #         \"format\": \"json\",\n",
    "    #         #\"num_rollout_workers\": 63,\n",
    "    #         #\"num_cpus_per_worker\": 0.5,\n",
    "    #     }\n",
    "    # )\n",
    "    .offline_data(input_=\"/DATA/pendulum-out/output-2023-02-11_15-20-26_worker-0_0.json\")\n",
    "    .evaluation(\n",
    "        evaluation_num_workers=1,\n",
    "        evaluation_interval=1,\n",
    "        evaluation_duration=10,\n",
    "        evaluation_parallel_to_training=False,\n",
    "        # evaluation_config=cql.CQLConfig.overrides(input_=\"sampler\"),\n",
    "        evaluation_config={\"input\":\"/DATA/pendulum-out/output-2023-02-11_15-20-26_worker-0_0.json\"}, # sampler\n",
    "        # off_policy_estimation_methods={\n",
    "        #     \"is\": {\"type\": ImportanceSampling},\n",
    "        #     \"wis\": {\"type\": WeightedImportanceSampling},\n",
    "        #     \"dm_fqe\": {\n",
    "        #         \"type\": DirectMethod,\n",
    "        #         \"q_model_config\": {\"type\": FQETorchModel, \"polyak_coef\": 0.05},\n",
    "        #         \"epsilon_greedy\": 0.0,\n",
    "        #     },\n",
    "        #     \"dr_fqe\": {\n",
    "        #         \"type\": DoublyRobust,\n",
    "        #         \"q_model_config\": {\"type\": FQETorchModel, \"polyak_coef\": 0.05},\n",
    "        #         \"epsilon_greedy\": 0.0,\n",
    "        #     },\n",
    "        # },\n",
    "    )\n",
    "    )\n",
    "# num_workers: 0\n",
    "#         num_gpus: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bb76d1b-01ff-455b-a01f-b394dde6b786",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoublyRobust\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfqe_torch_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FQETorchModel\n\u001b[1;32m      5\u001b[0m estimator \u001b[38;5;241m=\u001b[39m DoublyRobust(\n\u001b[0;32m----> 6\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[43malgo\u001b[49m\u001b[38;5;241m.\u001b[39mget_policy(),\n\u001b[1;32m      7\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m      8\u001b[0m     q_model_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: FQETorchModel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iters\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m160\u001b[39m},\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train estimator's Q-model; only required for DM and DR estimators\u001b[39;00m\n\u001b[1;32m     12\u001b[0m reader \u001b[38;5;241m=\u001b[39m JsonReader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/cartpole-out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.rllib.offline.estimators import DoublyRobust\n",
    "from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel\n",
    "\n",
    "estimator = DoublyRobust(\n",
    "    policy=algo.get_policy(),\n",
    "    gamma=0.99,\n",
    "    q_model_config={\"type\": FQETorchModel, \"n_iters\": 160},\n",
    ")\n",
    "\n",
    "# Train estimator's Q-model; only required for DM and DR estimators\n",
    "reader = JsonReader(\"/tmp/cartpole-out\")\n",
    "for _ in range(100):\n",
    "    batch = reader.next()\n",
    "    print(estimator.train(batch))\n",
    "    # {'loss': ...}\n",
    "\n",
    "reader = JsonReader(\"/tmp/cartpole-eval\")\n",
    "# Compute off-policy estimates\n",
    "for _ in range(100):\n",
    "    batch = reader.next()\n",
    "    print(estimator.estimate(batch))\n",
    "    # {'v_behavior': ..., 'v_target': ..., 'v_gain': ...,\n",
    "    # 'v_behavior_std': ..., 'v_target_std': ..., 'v_delta': ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2d5ff14-891b-4c84-a5b4-c1878cb25394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-11 15:56:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:25.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>176.8/503.2 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/1 GPUs, 0.0/449.86 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>CQL_Pendulum-v1_dff0b_00000</td><td>TERMINATED</td><td>172.17.0.2:1293210</td><td style=\"text-align: right;\">  1000</td><td style=\"text-align: right;\">         1124.32</td><td style=\"text-align: right;\">95604000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 15:56:04,472\tINFO tune.py:762 -- Total run time: 1165.86 seconds (1165.05 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.result_grid.ResultGrid at 0x7f0b4498e4f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# evaluation_parallel_to_training should be False b/c iterations are very long\n",
    "# and this would cause evaluation to lag one iter behind training.\n",
    "\n",
    "# Check, whether we can learn from the given file in `num_iterations`\n",
    "# iterations, up to a reward of `min_reward`.\n",
    "\n",
    "\n",
    "from ray import air, tune\n",
    "stop_iters = 1000\n",
    "stop_reward = -300\n",
    "\n",
    "# Test for torch framework (tf not implemented yet).\n",
    "# cql_algorithm = cql.CQL(config=config)\n",
    "# learnt = False\n",
    "# for i in range(num_iterations):\n",
    "#     print(f\"Iter {i}\")\n",
    "#     eval_results = cql_algorithm.train().get(\"evaluation\")\n",
    "#     if eval_results:\n",
    "#         print(\"... R={}\".format(eval_results[\"episode_reward_mean\"]))\n",
    "#         # Learn until some reward is reached on an actual live env.\n",
    "#         if eval_results[\"episode_reward_mean\"] >= min_reward:\n",
    "#             # Test passed gracefully.\n",
    "#             if as_test:\n",
    "#                 print(\"Test passed after {} iterations.\".format(i))\n",
    "#                 quit(0)\n",
    "#             learnt = True\n",
    "#             break\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": stop_iters,\n",
    "    \"evaluation/episode_reward_mean\": stop_reward,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    'CQL', param_space=config, \n",
    "    run_config=air.RunConfig(stop=stop, verbose=1, local_dir=ray_result_logdir,\n",
    "                                                      checkpoint_config=air.CheckpointConfig(num_to_keep=4, checkpoint_frequency = 10, checkpoint_score_attribute = 'episode_reward_mean')\n",
    "                                                )\n",
    ")\n",
    "tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78bed6f3-cbc3-46ad-bf84-ec705d90bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space =gym.spaces.Box(low=0, high=1, shape=(1,3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d7f797-d600-4f94-a8be-1878c4f8eb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2daed4d-7d93-459c-a46a-41eaa1e08128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get policy and model.\n",
    "cql_policy = cql_algorithm.get_policy()\n",
    "cql_model = cql_policy.model\n",
    "\n",
    "# If you would like to query CQL's learnt Q-function for arbitrary\n",
    "# (cont.) actions, do the following:\n",
    "obs_batch = torch.from_numpy(np.random.random(size=(5, 3)))\n",
    "action_batch = torch.from_numpy(np.random.random(size=(5, 1)))\n",
    "q_values = cql_model.get_q_values(obs_batch, action_batch)[0]\n",
    "# If you are using the \"twin_q\", there'll be 2 Q-networks and\n",
    "# we usually consider the min of the 2 outputs, like so:\n",
    "twin_q_values = cql_model.get_twin_q_values(obs_batch, action_batch)[0]\n",
    "final_q_values = torch.min(q_values, twin_q_values)[0]\n",
    "print(f\"final_q_values={final_q_values.detach().numpy()}\")\n",
    "\n",
    "# Example on how to do evaluation on the trained Algorithm.\n",
    "# using the data from our buffer.\n",
    "# Get a sample (MultiAgentBatch).\n",
    "\n",
    "batch = synchronous_parallel_sample(worker_set=cql_algorithm.workers)\n",
    "batch = convert_ma_batch_to_sample_batch(batch)\n",
    "obs = torch.from_numpy(batch[\"obs\"])\n",
    "# Pass the observations through our model to get the\n",
    "# features, which then to pass through the Q-head.\n",
    "model_out, _ = cql_model({\"obs\": obs})\n",
    "# The estimated Q-values from the (historic) actions in the batch.\n",
    "q_values_old = cql_model.get_q_values(\n",
    "    model_out, torch.from_numpy(batch[\"actions\"])\n",
    ")[0]\n",
    "# The estimated Q-values for the new actions computed by our policy.\n",
    "actions_new = cql_policy.compute_actions_from_input_dict({\"obs\": obs})[0]\n",
    "q_values_new = cql_model.get_q_values(model_out, torch.from_numpy(actions_new))[0]\n",
    "print(f\"Q-val batch={q_values_old.detach().numpy()}\")\n",
    "print(f\"Q-val policy={q_values_new.detach().numpy()}\")\n",
    "\n",
    "cql_algorithm.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a688ac-b5e1-4a0a-8c11-94fca1b4e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cql.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
