{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a0818-02ab-465a-be54-1440896fd474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ray._private.utils\n",
    "\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_builder = SampleBatchBuilder()  # or MultiAgentSampleBatchBuilder\n",
    "    writer = JsonWriter(\n",
    "        os.path.join('.', \"demo-out\")\n",
    "    )\n",
    "\n",
    "    # You normally wouldn't want to manually create sample batches if a\n",
    "    # simulator is available, but let's do it anyways for example purposes:\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "    # RLlib uses preprocessors to implement transforms such as one-hot encoding\n",
    "    # and flattening of tuple and dict observations. For CartPole a no-op\n",
    "    # preprocessor is used, but this may be relevant for more complex envs.\n",
    "    prep = get_preprocessor(env.observation_space)(env.observation_space)\n",
    "    print(\"The preprocessor is\", prep)\n",
    "\n",
    "    for eps_id in range(100):\n",
    "        obs = env.reset()\n",
    "        prev_action = np.zeros_like(env.action_space.sample())\n",
    "        prev_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, rew, done, info = env.step(action)\n",
    "            batch_builder.add_values(\n",
    "                t=t,\n",
    "                eps_id=eps_id,\n",
    "                agent_index=0,\n",
    "                obs=prep.transform(obs),\n",
    "                actions=action,\n",
    "                action_prob=1.0,  # put the true action probability here\n",
    "                action_logp=0.0,\n",
    "                rewards=rew,\n",
    "                prev_actions=prev_action,\n",
    "                prev_rewards=prev_reward,\n",
    "                dones=done,\n",
    "                infos=info,\n",
    "                new_obs=prep.transform(new_obs),\n",
    "            )\n",
    "            obs = new_obs\n",
    "            prev_action = action\n",
    "            prev_reward = rew\n",
    "            t += 1\n",
    "        writer.write(batch_builder.build_and_reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4487d-c438-40b3-a4ac-e111b7f5a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rllib train \\\n",
    "    --run=PG \\\n",
    "    --env=Pendulum-v1 \\\n",
    "     --framework=torch \\\n",
    "    --config='{\"output\": \"/DATA/pendulum-out\", \"output_max_file_size\": 100}' \\\n",
    "    --stop='{\"timesteps_total\": 100}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c573d-215e-434f-a483-57e58dc196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "\n",
    "from ray.rllib.policy.sample_batch import convert_ma_batch_to_sample_batch\n",
    "from ray.rllib.algorithms import cql as cql\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.execution.rollout_ops import (\n",
    "    synchronous_parallel_sample,\n",
    ")\n",
    "from ray.rllib.offline.estimators import (\n",
    "    ImportanceSampling,\n",
    "    WeightedImportanceSampling,\n",
    "    DirectMethod,\n",
    "    DoublyRobust,\n",
    ")\n",
    "from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel\n",
    "\n",
    "\n",
    "torch, _ = try_import_torch()\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     \"--as-test\",\n",
    "#     action=\"store_true\",\n",
    "#     help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "#     \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--stop-iters\", type=int, default=5, help=\"Number of iterations to train.\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--stop-reward\", type=float, default=50.0, help=\"Reward at which we stop training.\"\n",
    "# )\n",
    "hcmTz = pytz.timezone(\"Asia/Ho_Chi_Minh\") \n",
    "date = datetime.datetime.now(hcmTz).strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "ray_result_logdir = '/DATA/l5kit/ray_results/' + date\n",
    "\n",
    "\n",
    "stop_iters = 5\n",
    "stop_reward=50\n",
    "as_test=True\n",
    "runs = 'CQL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e2de7-e1d0-436b-b129-e5be31d71dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ray\n",
    "ray.init(num_cpus=64, ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553d1ea-ee9c-4fb8-be1c-b69872d9f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    cql.CQLConfig()\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(num_rollout_workers=62)\n",
    "    .resources(num_gpus = 1)\n",
    "    .training(\n",
    "        n_step=3,\n",
    "        bc_iters=0,\n",
    "        clip_actions=False,\n",
    "        tau=0.005,\n",
    "        target_entropy=\"auto\",\n",
    "        q_model_config={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        policy_model_config={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        optimization_config={\n",
    "            \"actor_learning_rate\": 3e-4,\n",
    "            \"critic_learning_rate\": 3e-4,\n",
    "            \"entropy_learning_rate\": 3e-4,\n",
    "        },\n",
    "        train_batch_size=256,\n",
    "        target_network_update_freq=1,\n",
    "        num_steps_sampled_before_learning_starts=256,\n",
    "    )\n",
    "    .reporting(min_train_timesteps_per_iteration=1000)\n",
    "    .debugging(log_level=\"INFO\")\n",
    "    .environment(normalize_actions=True, env=\"Pendulum-v1\")\n",
    "    # .offline_data(\n",
    "    #     input_config={\n",
    "    #         \"paths\": [\"./demo-out/output-2023-02-11_06-03-52_worker-0_0.json\"], #tests/data/pendulum/enormous.zip\n",
    "    #         \"format\": \"json\",\n",
    "    #         #\"num_rollout_workers\": 63,\n",
    "    #         #\"num_cpus_per_worker\": 0.5,\n",
    "    #     }\n",
    "    # )\n",
    "    .offline_data(input_=\"/DATA/pendulum-out/output-2023-02-11_15-20-26_worker-0_0.json\")\n",
    "    .evaluation(\n",
    "        evaluation_num_workers=1,\n",
    "        evaluation_interval=1,\n",
    "        evaluation_duration=10,\n",
    "        evaluation_parallel_to_training=False,\n",
    "        # evaluation_config=cql.CQLConfig.overrides(input_=\"sampler\"),\n",
    "        evaluation_config={\"input\":\"/DATA/pendulum-out/output-2023-02-11_15-20-26_worker-0_0.json\"}, # sampler\n",
    "        # off_policy_estimation_methods={\n",
    "        #     \"is\": {\"type\": ImportanceSampling},\n",
    "        #     \"wis\": {\"type\": WeightedImportanceSampling},\n",
    "        #     \"dm_fqe\": {\n",
    "        #         \"type\": DirectMethod,\n",
    "        #         \"q_model_config\": {\"type\": FQETorchModel, \"polyak_coef\": 0.05},\n",
    "        #         \"epsilon_greedy\": 0.0,\n",
    "        #     },\n",
    "        #     \"dr_fqe\": {\n",
    "        #         \"type\": DoublyRobust,\n",
    "        #         \"q_model_config\": {\"type\": FQETorchModel, \"polyak_coef\": 0.05},\n",
    "        #         \"epsilon_greedy\": 0.0,\n",
    "        #     },\n",
    "        # },\n",
    "    )\n",
    "    )\n",
    "# num_workers: 0\n",
    "#         num_gpus: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb76d1b-01ff-455b-a01f-b394dde6b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.rllib.offline.estimators import DoublyRobust\n",
    "from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel\n",
    "\n",
    "estimator = DoublyRobust(\n",
    "    policy=algo.get_policy(),\n",
    "    gamma=0.99,\n",
    "    q_model_config={\"type\": FQETorchModel, \"n_iters\": 160},\n",
    ")\n",
    "\n",
    "# Train estimator's Q-model; only required for DM and DR estimators\n",
    "reader = JsonReader(\"/tmp/cartpole-out\")\n",
    "for _ in range(100):\n",
    "    batch = reader.next()\n",
    "    print(estimator.train(batch))\n",
    "    # {'loss': ...}\n",
    "\n",
    "reader = JsonReader(\"/tmp/cartpole-eval\")\n",
    "# Compute off-policy estimates\n",
    "for _ in range(100):\n",
    "    batch = reader.next()\n",
    "    print(estimator.estimate(batch))\n",
    "    # {'v_behavior': ..., 'v_target': ..., 'v_gain': ...,\n",
    "    # 'v_behavior_std': ..., 'v_target_std': ..., 'v_delta': ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5ff14-891b-4c84-a5b4-c1878cb25394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluation_parallel_to_training should be False b/c iterations are very long\n",
    "# and this would cause evaluation to lag one iter behind training.\n",
    "\n",
    "# Check, whether we can learn from the given file in `num_iterations`\n",
    "# iterations, up to a reward of `min_reward`.\n",
    "\n",
    "\n",
    "from ray import air, tune\n",
    "stop_iters = 1000\n",
    "stop_reward = -300\n",
    "\n",
    "# Test for torch framework (tf not implemented yet).\n",
    "# cql_algorithm = cql.CQL(config=config)\n",
    "# learnt = False\n",
    "# for i in range(num_iterations):\n",
    "#     print(f\"Iter {i}\")\n",
    "#     eval_results = cql_algorithm.train().get(\"evaluation\")\n",
    "#     if eval_results:\n",
    "#         print(\"... R={}\".format(eval_results[\"episode_reward_mean\"]))\n",
    "#         # Learn until some reward is reached on an actual live env.\n",
    "#         if eval_results[\"episode_reward_mean\"] >= min_reward:\n",
    "#             # Test passed gracefully.\n",
    "#             if as_test:\n",
    "#                 print(\"Test passed after {} iterations.\".format(i))\n",
    "#                 quit(0)\n",
    "#             learnt = True\n",
    "#             break\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": stop_iters,\n",
    "    \"evaluation/episode_reward_mean\": stop_reward,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    'CQL', param_space=config, \n",
    "    run_config=air.RunConfig(stop=stop, verbose=1, local_dir=ray_result_logdir,\n",
    "                                                      checkpoint_config=air.CheckpointConfig(num_to_keep=4, checkpoint_frequency = 10, checkpoint_score_attribute = 'episode_reward_mean')\n",
    "                                                )\n",
    ")\n",
    "tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bed6f3-cbc3-46ad-bf84-ec705d90bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space =gym.spaces.Box(low=0, high=1, shape=(1,3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7f797-d600-4f94-a8be-1878c4f8eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2daed4d-7d93-459c-a46a-41eaa1e08128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get policy and model.\n",
    "cql_policy = cql_algorithm.get_policy()\n",
    "cql_model = cql_policy.model\n",
    "\n",
    "# If you would like to query CQL's learnt Q-function for arbitrary\n",
    "# (cont.) actions, do the following:\n",
    "obs_batch = torch.from_numpy(np.random.random(size=(5, 3)))\n",
    "action_batch = torch.from_numpy(np.random.random(size=(5, 1)))\n",
    "q_values = cql_model.get_q_values(obs_batch, action_batch)[0]\n",
    "# If you are using the \"twin_q\", there'll be 2 Q-networks and\n",
    "# we usually consider the min of the 2 outputs, like so:\n",
    "twin_q_values = cql_model.get_twin_q_values(obs_batch, action_batch)[0]\n",
    "final_q_values = torch.min(q_values, twin_q_values)[0]\n",
    "print(f\"final_q_values={final_q_values.detach().numpy()}\")\n",
    "\n",
    "# Example on how to do evaluation on the trained Algorithm.\n",
    "# using the data from our buffer.\n",
    "# Get a sample (MultiAgentBatch).\n",
    "\n",
    "batch = synchronous_parallel_sample(worker_set=cql_algorithm.workers)\n",
    "batch = convert_ma_batch_to_sample_batch(batch)\n",
    "obs = torch.from_numpy(batch[\"obs\"])\n",
    "# Pass the observations through our model to get the\n",
    "# features, which then to pass through the Q-head.\n",
    "model_out, _ = cql_model({\"obs\": obs})\n",
    "# The estimated Q-values from the (historic) actions in the batch.\n",
    "q_values_old = cql_model.get_q_values(\n",
    "    model_out, torch.from_numpy(batch[\"actions\"])\n",
    ")[0]\n",
    "# The estimated Q-values for the new actions computed by our policy.\n",
    "actions_new = cql_policy.compute_actions_from_input_dict({\"obs\": obs})[0]\n",
    "q_values_new = cql_model.get_q_values(model_out, torch.from_numpy(actions_new))[0]\n",
    "print(f\"Q-val batch={q_values_old.detach().numpy()}\")\n",
    "print(f\"Q-val policy={q_values_new.detach().numpy()}\")\n",
    "\n",
    "cql_algorithm.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a688ac-b5e1-4a0a-8c11-94fca1b4e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cql.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
